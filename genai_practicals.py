# -*- coding: utf-8 -*-
"""GenAI_Practicals.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wnfh1-drp4f5DfuIwZq1xbAaAJ_hi37w
"""

##### Sentiment_Analysis  #####

from transformers import pipeline
classifier = pipeline(task = "sentiment-analysis")
review = "This is a very good movie"
result = classifier(review)
print(result)

from transformers import pipeline
classifier = pipeline(task = "sentiment-analysis")
review = "This is one of the worst movie"
result = classifier(review)
print(result)

##### Text_Summarization #####

text = """Generative AI (GenAI) is revolutionizing the way we create and interact with technology. Unlike traditional AI, which analyzes and classifies existing data, GenAI goes a step further—it creates. From drafting compelling text to generating lifelike images and composing original music, GenAI mimics human creativity using powerful models trained on vast datasets.
At the heart of this transformation are tools like OpenAI’s ChatGPT for language, DALL·E for images, and Sora for videos. These models can write code, design graphics, produce marketing content, and even simulate voices. Businesses are rapidly adopting GenAI to enhance productivity, accelerate innovation, and unlock new revenue streams.
But with great power comes great responsibility. As GenAI evolves, ethical considerations around misinformation, intellectual property, and bias are gaining importance. Ensuring transparency and responsible use is critical as we enter this new era of machine-assisted creativity.
GenAI isn’t just a tool—it’s a collaborator. It empowers individuals and organizations to think beyond limitations and create in ways previously unimaginable."""

from transformers import pipeline
summarizer = pipeline(task="summarization")
summary = summarizer(text)
print(summary)

text = """Generative AI (GenAI) is revolutionizing the way we create and interact with technology. Unlike traditional AI, which analyzes and classifies existing data, GenAI goes a step further—it creates. From drafting compelling text to generating lifelike images and composing original music, GenAI mimics human creativity using powerful models trained on vast datasets.
At the heart of this transformation are tools like OpenAI’s ChatGPT for language, DALL·E for images, and Sora for videos. These models can write code, design graphics, produce marketing content, and even simulate voices. Businesses are rapidly adopting GenAI to enhance productivity, accelerate innovation, and unlock new revenue streams.
But with great power comes great responsibility. As GenAI evolves, ethical considerations around misinformation, intellectual property, and bias are gaining importance. Ensuring transparency and responsible use is critical as we enter this new era of machine-assisted creativity.
GenAI isn’t just a tool—it’s a collaborator. It empowers individuals and organizations to think beyond limitations and create in ways previously unimaginable."""

from transformers import pipeline
summarizer = pipeline(task="summarization", min_length = 30, max_length =90)
summary = summarizer(text)
print(summary)

##### Question_Answering #####

from transformers import pipeline
classifier = pipeline(task="question-answering")
text = "Hugging Face is a company and open-source community that provides tools, libraries, and models for working with machine learning (ML) and natural language processing (NLP), especially in the field of Generative AI."
qus = "What is Hugging Face?"
result = classifier(context = text, question = qus)
print(result)

##### Translation_Task #####

#Translator for converting English to French
from transformers import pipeline
translation = pipeline(task="translation_en_to_fr")
text = "My name is Manohar Reddy"
print(translation(text))

##### Token_Classification  #####

#Token Classification like names, date, locations etc..
#ner = named entity recognisation
from transformers import pipeline
ner = pipeline("ner")
text = "Elon Musk Founded Tesla in 2023 and spacex in 2020"
print(ner(text))

#Token Classification like names, date, locations etc..
#ner = named entity recognisation
#grouped_entities >>for grouping the tokens
from transformers import pipeline
ner = pipeline("ner", grouped_entities = True)
text = "Elon Musk Founded Tesla in 2023 and spacex in 2020"
print(ner(text))

#Token Classification like names, date, locations etc..
#directly passing the trained model
#pos  >>for parts of speech
from transformers import pipeline
ner = pipeline(model = "vblagoje/bert-english-uncased-finetuned-pos")
text = "Elon Musk Founded Tesla in 2023 and spacex in 2020"
print(ner(text))

##### Fill_Mask #####

#fill mask // for filling blank
#model name passed
from transformers import pipeline
fill_mask = pipeline("fill-mask",model = "google-bert/bert-base-uncased")
text = "Elon Musk [MASK] Tesla in 2023 and spacex in 2020"
print(fill_mask(text))

#fill mask // for filling blank
#model name not passed
#In default model, it expects <mask>, isted of [MASK]
from transformers import pipeline
fill_mask = pipeline("fill-mask")
text = "Elon Musk <mask> Tesla in 2023 and spacex in 2020"
print(fill_mask(text))

##### Text_Generation #####

#for generating next words for the given prompt
from transformers import pipeline
generator = pipeline("text-generation", model="gpt2")
prompt = "Texas is a very big"
print(generator(prompt))

#Text Generation //for filling next words
#with maximum lenth of 50 token, including the provided tokens
#with one sequence
from transformers import pipeline
generator = pipeline("text-generation", model="gpt2")
prompt = "Texas is a very big"
print(generator(prompt,max_length=10, num_return_sequences=1))

#Text Generation //for filling next words
#with maximum lenth of 50 token, including the provided tokens
#with 3 sequence
from transformers import pipeline
generator = pipeline("text-generation", model="gpt2")
prompt = "Texas is a very big"
print(generator(prompt,max_length=5 , num_return_sequences=3))

##### Feature_Extraction #####

##### Extracting_Static_Embeddinds  #####

# return_tensors = "pt" >> for holding tensor >> pt =pytorch

from transformers import pipeline
text = "Citi bank is one of the popular bank in USA"
feature_excecutor = pipeline("feature-extraction")
results= feature_excecutor(text, return_tensors = "pt")
print(results)

# Extracting_Static_Embeddinds
# return_tensors = "pt" >> for holding tensor >> pt =pytorch
# print(results.shape) >> for printing no:of tokens

from transformers import pipeline
text = "Citi bank is one of the popular bank in USA"
feature_excecutor = pipeline("feature-extraction")
results= feature_excecutor(text, return_tensors = "pt")
print(results)
print(results.shape)

#Extracting static embeddings >>Passing model
#using facebook/bart-base

from transformers import pipeline
text = "Citi bank is one of the popular bank in USA"
feature_excecutor = pipeline("feature-extraction", model = "facebook/bart-base")
results= feature_excecutor(text, return_tensors = "pt")
print(results)
print(results.shape)

##### Zero Shot Classification #####

#Text Classification //sentiment-analysis

from transformers import pipeline
classifier = pipeline("text-classification")
text = "Apple has released new iphone with good performance and security"
result = classifier(text)
print(result)

#Text Classification //sentiment-analysis

#Zero_Shot_Classification >>For Providing our own lables


from transformers import pipeline
classifier = pipeline("zero-shot-classification")
text = "Apple has released new iphone with good performance and security"
lables = ["Technology","Banking","Polytics","Sports"]
result = classifier(text, lables)
print(result)

##### Flow  #####

###Extracting_Tokens

from transformers import pipeline
classifier = pipeline("sentiment-analysis")
review = "This is a very good movie"
classifier.tokenizer

print(classifier.tokenizer)

#Extracting Tokens, by padding o's at the end to match the length

#padding = True >>adds 0's at the end

from transformers import pipeline
classifier = pipeline("sentiment-analysis")
review = ["This is a very good movie","I like this movie"]
tokenizer = classifier.tokenizer
result = tokenizer(review, padding = True, return_tensors = "pt")
print(result)

### Extracting_Model

model = classifier.model
print(model)

### Extracting_Logits

model = classifier.model
out = model(result['input_ids'],result['attention_mask'])
print(out)

### End_to_End flow


from transformers import pipeline
classifier = pipeline("sentiment-analysis")
review = "This is a very good movie"

#tokens
tokenizer = classifier.tokenizer
print(tokenizer)

#model
model = classifier.model
print(model)


result = tokenizer(review, padding = True, return_tensors = "pt")
print(result)


print(result['input_ids'],result['attention_mask'])

#logits
output = model(result['input_ids'],result['attention_mask'])
print(output)